import os, math, random, argparse, time
from pathlib import Path
import numpy as np
import cv2
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

# ---------------------------
# Speed knobs (safe defaults)
# ---------------------------
torch.backends.cudnn.benchmark = True   # speed up fixed-size convs
torch.set_float32_matmul_precision("high")

# ---------------------------
# Dataset
# ---------------------------
class SEMTiles(Dataset):
    """
    PNG tiles named like:
        <INTEGER>__anything.png
    where <INTEGER> is the scale (nm/px). Examples:
        2870__page9_img2_x0_y0.png
        351__tile_0001.png
    Any file that doesn't start with "<digits>__" is ignored.
    """
    def __init__(self, root, size=224, cache=False):
        self.root  = Path(root)
        self.size  = size
        self.cache = cache
        self.items = []          # list[(Path, int nm_per_px)]
        self._cache = {}

        for p in sorted(self.root.glob("*.png")):
            name = p.name
            if "__" not in name:
                continue
            prefix = name.split("__", 1)[0]
            if not prefix.isdigit():
                continue
            nm = int(prefix)
            self.items.append((p, nm))

        if not self.items:
            raise RuntimeError(f"No tiles with '<digits>__*.png' found in {self.root}")

    def __len__(self): return len(self.items)

    def _load_img(self, path: Path):
        im = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)
        if im is None:
            raise RuntimeError(f"Failed to read {path}")
        if self.size and (im.shape[0] != self.size or im.shape[1] != self.size):
            im = cv2.resize(im, (self.size, self.size), interpolation=cv2.INTER_AREA)
        # [H,W] -> [1,H,W], normalize to [-1,1]
        im = torch.from_numpy(im).float().unsqueeze(0) / 127.5 - 1.0
        return im

    def __getitem__(self, idx):
        path, nm = self.items[idx]
        if self.cache and path in self._cache:
            x = self._cache[path]
        else:
            x = self._load_img(path)
            if self.cache:
                self._cache[path] = x
        # simple scalar condition: log(nm) mapped to ~[-1,1]
        cond = torch.tensor([np.log(max(nm,1)) / 8.0], dtype=torch.float32)
        return x, cond

# ---------------------------
# Tiny UNet for 224x224 gray
# ---------------------------
class Block(nn.Module):
    def __init__(self, c_in, c_out, t_dim):
        super().__init__()
        self.conv = nn.Sequential(
            nn.GroupNorm(8, c_in), nn.SiLU(), nn.Conv2d(c_in, c_out, 3, padding=1),
            nn.GroupNorm(8, c_out), nn.SiLU(), nn.Conv2d(c_out, c_out, 3, padding=1)
        )
        self.to_t = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, c_out))
        self.res  = (c_in == c_out)

    def forward(self, x, t):
        h = self.conv[0](x); h = self.conv[1](h); h = self.conv[2](h)
        # add time
        h = h + self.to_t(t).unsqueeze(-1).unsqueeze(-1)
        h = self.conv[3](h); h = self.conv[4](h); h = self.conv[5](h)
        return h + x if self.res else h

class Down(nn.Module):
    def __init__(self, c_in, c_out, t_dim):
        super().__init__()
        self.pool = nn.Conv2d(c_in, c_in, 3, stride=2, padding=1)
        self.block = Block(c_in, c_out, t_dim)
    def forward(self, x, t): return self.block(self.pool(x), t)

class Up(nn.Module):
    def __init__(self, c_in, c_out, t_dim):
        super().__init__()
        self.up = nn.ConvTranspose2d(c_in, c_in, 4, stride=2, padding=1)
        self.block = Block(c_in, c_out, t_dim)
    def forward(self, x, t): return self.block(self.up(x), t)

class UNet(nn.Module):
    def __init__(self, c=48, t_dim=64):  # width c is CLI arg
        super().__init__()
        self.t_mlp = nn.Sequential(
            nn.Linear(1, t_dim), nn.SiLU(),
            nn.Linear(t_dim, t_dim), nn.SiLU()
        )
        self.inp = nn.Conv2d(1, c, 3, padding=1)
        self.d1 = Down(c, c, t_dim)     # 224 -> 112
        self.d2 = Down(c, c*2, t_dim)   # 112 -> 56
        self.d3 = Down(c*2, c*2, t_dim) # 56 -> 28
        self.mid = Block(c*2, c*2, t_dim)
        self.u3 = Up(c*2, c*2, t_dim)   # 28 -> 56
        self.u2 = Up(c*2, c, t_dim)     # 56 -> 112
        self.u1 = Up(c, c, t_dim)       # 112 -> 224
        self.out = nn.Conv2d(c, 1, 3, padding=1)

    def forward(self, x, cond, t_frac):
        # combine diffusion time and cond into a single t vector
        t = torch.cat([t_frac.unsqueeze(-1), cond], dim=-1).mean(dim=-1, keepdim=True)  # [B,1]
        temb = self.t_mlp(t)  # [B,t_dim]

        x = self.inp(x)
        x = self.d1(x, temb)
        x = self.d2(x, temb)
        x = self.d3(x, temb)
        x = self.mid(x, temb)
        x = self.u3(x, temb)
        x = self.u2(x, temb)
        x = self.u1(x, temb)
        return self.out(x)

# ---------------------------
# Diffusion utilities
# ---------------------------
class DDPM:
    def __init__(self, T=1000, beta1=1e-4, beta2=0.02, device="cuda"):
        self.T = T
        betas = torch.linspace(beta1, beta2, T, device=device)
        alphas = 1.0 - betas
        self.a_bar = torch.cumprod(alphas, dim=0)
        self.sqrt_ab = torch.sqrt(self.a_bar)
        self.sqrt_1mab = torch.sqrt(1.0 - self.a_bar)

    def add_noise(self, x0, t, eps=None):
        if eps is None: eps = torch.randn_like(x0)
        a = self.sqrt_ab[t][:, None, None, None]
        b = self.sqrt_1mab[t][:, None, None, None]
        return a * x0 + b * eps, eps

# ---------------------------
# Training
# ---------------------------
def seed_all(s=1337):
    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)

def to_device(batch, device, memory_format=None):
    x, c = batch
    x = x.to(device, non_blocking=True)
    if memory_format == torch.channels_last:
        x = x.contiguous(memory_format=torch.channels_last)
    return x, c.to(device, non_blocking=True)

def train(args):
    seed_all()
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")

    # dataset / loader
    ds = SEMTiles(args.data, size=224, cache=args.cache)
    loader = DataLoader(
        ds,
        batch_size=args.batch,
        shuffle=True,
        num_workers=args.workers,
        pin_memory=True,
        persistent_workers=(args.workers > 0),
        drop_last=True
    )

    # model
    net = UNet(c=args.width).to(device)
    if args.channels_last:
        net = net.to(memory_format=torch.channels_last)
    if args.amp:
        scaler = torch.amp.GradScaler(device if device != "cpu" else "cpu")
    else:
        scaler = None

    # opt & diffusion
    opt = torch.optim.AdamW(net.parameters(), lr=args.lr, betas=(0.9, 0.99), fused=True)
    ddpm = DDPM(T=args.steps, device=device)

    # training loop
    global_step = 0
    for epoch in range(1, args.epochs+1):
        net.train()
        pbar = tqdm(loader, total=min(len(loader), args.epoch_steps or len(loader)), ncols=100,
                    desc=f"epoch {epoch}/{args.epochs}")
        running = 0.0
        for it, batch in enumerate(pbar):
            if args.epoch_steps and it >= args.epoch_steps:
                break

            x0, cond = to_device(batch, device, torch.channels_last if args.channels_last else None)
            B = x0.size(0)
            t = torch.randint(0, args.steps, (B,), device=device)
            xt, eps = ddpm.add_noise(x0, t)

            with torch.autocast(device_type=device, dtype=torch.float16 if args.amp else torch.bfloat16, enabled=args.amp):
                pred = net(xt, cond, t.float()/args.steps)
                loss = torch.mean((pred - eps)**2)

            if scaler:
                scaler.scale(loss).backward()
                scaler.step(opt)
                scaler.update()
                opt.zero_grad(set_to_none=True)
            else:
                loss.backward()
                opt.step()
                opt.zero_grad(set_to_none=True)

            running += loss.item()
            global_step += 1
            pbar.set_postfix_str(f"loss={running/(it+1):.4f}")

        # save each epoch
        Path(args.out).mkdir(parents=True, exist_ok=True)
        torch.save(net.state_dict(), str(Path(args.out)/f"sem_ddpm_e{epoch}.pt"))

# ---------------------------
# CLI
# ---------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--data", required=True, help="Folder with <digits>__*.png tiles")
    p.add_argument("--out", default="models_sem_fast")
    p.add_argument("--epochs", type=int, default=12)
    p.add_argument("--batch", type=int, default=48)
    p.add_argument("--width", type=int, default=48, help="UNet base channels")
    p.add_argument("--steps", type=int, default=1000, help="Diffusion steps T")
    p.add_argument("--lr", type=float, default=2e-4)
    p.add_argument("--workers", type=int, default=8)
    p.add_argument("--cache", action="store_true", help="Cache images in RAM")
    p.add_argument("--amp", action="store_true", help="Mixed precision")
    p.add_argument("--channels_last", action="store_true")
    p.add_argument("--epoch_steps", type=int, default=300, help="Cap steps per epoch for speed (0=all)")
    return p.parse_args()

if __name__ == "__main__":
    args = parse_args()
    train(args)
